All benchmarks on a 2 core Intel Core i5 at 1.8GHz.

no parallelism:

{100384588,
 [{wildcat,0.62886},
  {diabolical,75.74654},
  {vegard_hanssen,194.76248999999999},
  {challenge,13.178360000000001},
  {challenge1,648.03425},
  {extreme,15.13585},
  {seventeen,56.359339999999996}]}


parallel benchmarks:

{69662750,
 [{wildcat,4.18391},
  {extreme,26.82369},
  {challenge,53.24754},
  {seventeen,104.5336},
  {diabolical,157.2561},
  {vegard_hanssen,234.93266},
  {challenge1,696.61771}]}

overall speedup: 1.44
challenge: 0.25
challenge1: 0.93
diabolical: 0.48
extreme: 0.56
seventeen: 0.54
vegard_hanssen: 0.83
wildcat: 0.15

As can be seen in the graph par_bench.png we achieve nice parallelism
for about half the time, but the process handling the challenge1
sudoku is more than twice as long, because of switching between solving
different puzzles.


parallel guesses:

{79782203,
 [{wildcat,0.6725599999999999},
  {diabolical,36.911339999999996},
  {vegard_hanssen,86.60652},
  {challenge,7.07066},
  {challenge1,579.6075500000001},
  {extreme,30.71952},
  {seventeen,56.23366}]}

overall speedup: 1.26
challenge: 1.86
challenge1: 1.12
diabolical: 2.05
extreme: 0.49
seventeen: 1.00
vegard_hanssen: 2.25
wildcat: 0.94
geometric mean: 1.24

This is parallel guessing only for the first possible guesses, the
depth can be increased to yield better results for some of the
problems (for example challenge1 can be made quicker) but it then gets
worse on other leading to a worse overall speedup.



parallel rows:


When parallelising the refine_row function, lots of processes get
spawned (27 for each run of refine), because there are nine real rows,
nine "column rows" and nine "box rows". In fact the processes were too
many for percept to be able to analyse the data, and this is a good
example of too fine granularity when parallelising. As we see, the
total running time doubled, compared to the sequential benchmarking
of the solver.




parallel rows/columns/boxes:


We tried parallelising the refining of the rows, columns and boxes of
a sudoku puzzle. We only got it running for the wildcat puzzle so far,
but it ran about 11 times slower than the sequential solver, which is
probably partly because of the increased work. The sequential solver
first refines boxes, then takes the refined version, refines the
columns, take the refined version and refines the rows, making the
data to work at smaller at each step. When parallelising these three,
they all have to work at the iteration's initial sudoku puzzle, i.e.
two of them work on potentially bigger data. After all three are done,
there is also a need of converting lists to sets, taking out the
intersection and lastly converting back from sets to lists.
